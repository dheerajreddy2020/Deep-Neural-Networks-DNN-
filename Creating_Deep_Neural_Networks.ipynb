{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Creating-Deep-Neural-Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHfzKwAHkyQ06GV7ez8fa/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dheerajreddy2020/Deep-Neural-Networks-DNN-/blob/master/Creating_Deep_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTp9jOR9daqg",
        "colab_type": "text"
      },
      "source": [
        "# Understanding the building blocks of Deep Neural Networks\n",
        "* This notebook helps you in understanding how to develop a deep neural network.\n",
        "* What goes on behind the Dense commands in Keras/Tensorflow.\n",
        "* How to optimize the parameters using gradient descent method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWgwk2JmryBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZY069kyYDmu",
        "colab_type": "text"
      },
      "source": [
        "## Create activation functions and their gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JTzmIoBsWyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  return (1/(1+np.exp(-z)))\n",
        "\n",
        "def relu(Z):\n",
        "  Z[Z<0]=0\n",
        "  return Z\n",
        "\n",
        "def sigmoid_backward(Z):\n",
        "  return sigmoid(Z)*(1-sigmoid(Z))\n",
        "\n",
        "def relu_backward(Z):\n",
        "  Z[Z<=0]=0\n",
        "  Z[Z>0]=1\n",
        "  return Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZQSIsIgTLuB",
        "colab_type": "text"
      },
      "source": [
        "## 1. Initializing the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPdR7RmTGpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This function works similar to kernel initializer = 'glorot_normal' and bias_initilizer= 'zeros' in keras\n",
        "def initialize_parameters(dims):\n",
        "  '''\n",
        "  input an array of dimensions in each layer \n",
        "  Eg dims= [10,8,5,2] denotes 10 input parameters, \n",
        "  8 neurons in 1st hidden layer, \n",
        "  5 neurons in 2nd hidden layer,\n",
        "  2 neurons in output layer.\n",
        "  Returns a dictionary of parameters W(Kernel) & b(bias)\n",
        "  '''\n",
        "  params={}\n",
        "  for i in range(1,len(dims)):\n",
        "    params[\"W\"+str(i)]=np.random.randn(dims[i],dims[i-1])/np.sqrt(dims[i-1])\n",
        "    params[\"b\"+str(i)]=np.zeros((dims[i],1))\n",
        "  return params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HtznzsCUPqX",
        "colab_type": "text"
      },
      "source": [
        "##2. Create forward Propagation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgjkUjnGUMFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_forward(X,params):\n",
        "  '''\n",
        "  X is the input parameters\n",
        "  params is a dictionary of parameters \n",
        "  Returns dictionaries of outputs and activations for all the layers Z &\n",
        "  '''\n",
        "  net_len=int(len(params)/2)\n",
        "  Z={}\n",
        "  A={}\n",
        "  A[str(0)]=X\n",
        "  for i in range(1,net_len):\n",
        "    Z[str(i)]=np.dot(params[\"W\"+str(i)],A[str(i-1)])+params[\"b\"+str(i)]\n",
        "    A[str(i)]=relu(Z[str(i)])\n",
        "  \n",
        "  Z[str(net_len)]=np.dot(params[\"W\"+str(net_len)],A[str(net_len-1)])+params[\"b\"+str(net_len)]\n",
        "  A[str(net_len)]=sigmoid(Z[str(net_len)])\n",
        "  return A,Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5BPHLUiU18A",
        "colab_type": "text"
      },
      "source": [
        "## 3. Calculating Cost/ loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opUAH_L7VXKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cost(Y_pred,Y_act,n_outputs):\n",
        "  '''\n",
        "  Make sure the first argument is the predicted Y and 2nd argument is actual Y\n",
        "  Returns Cost or loss of the current prediction\n",
        "  '''\n",
        "  m=Y_pred.shape[1]\n",
        "  if n_outputs>1:\n",
        "    cost=-1/m*np.sum(Y_act*np.log(Y_pred))\n",
        "  else:\n",
        "    cost=-1/m*np.sum(Y_act*np.log(Y_pred)+(1-Y_act)*np.log(1-Y_pred))\n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU81BvwHVnuQ",
        "colab_type": "text"
      },
      "source": [
        "## 4. Create Backward propagation functions\n",
        "* This is a very important step which helps in updating the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24QzU4XkWRPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_backward(Y,A,Z,params):\n",
        "  '''\n",
        "  Y- Actual outputs in the last layer\n",
        "  Give dictionaries of \n",
        "  A- Dictionary with activations of each layer\n",
        "  Z- Estimated values before activation\n",
        "  params- Parameters dictionary consisting of W and b\n",
        "  Returns gradients (grads) and cache(consists of da and dz)\n",
        "  '''\n",
        "  dz={}\n",
        "  da={}\n",
        "  grads={}\n",
        "  l=len(Z)\n",
        "  m=Y.shape[1]\n",
        "  dz[str(l)]=A[str(l)]-Y\n",
        "  for i in reversed(range(1,l+1)):\n",
        "    grads['dW'+str(i)]=1/m*np.dot(dz[str(i)],A[str(i-1)].T)\n",
        "    grads['db'+str(i)]=1/m*np.sum(dz[str(i)],axis=1,keepdims=True)\n",
        "    if i>1:\n",
        "      da[str(i-1)]=np.dot(params['W'+str(i)].T,dz[str(i)])\n",
        "      dz[str(i-1)]=da[str(i-1)]*relu_backward(Z[str(i-1)])\n",
        "  cache=da,dz\n",
        "  return grads,cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOijplKxXgwP",
        "colab_type": "text"
      },
      "source": [
        "## 5. Create function to update all the parameters\n",
        "* Here a Gradient design optimization method is used to optimize the parameters for each step\n",
        "* However there are many advanced optimization methods.\n",
        "* RMS Prop and Adam optimizer are frequently used in general"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLWzXZFNt-RQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_params(params,grads,alpha,n_layers):\n",
        "  '''\n",
        "  params- Parameters dictionary\n",
        "  grads- grads dictionary\n",
        "  alpha- learning rate\n",
        "  n_layers- number of layers of deep learning network\n",
        "  Returns dictionary of updated paramters\n",
        "  '''\n",
        "  for i in range(1,n_layers+1):\n",
        "    params[\"W\"+str(i)]=params[\"W\"+str(i)]-alpha*grads['dW'+str(i)]\n",
        "    params[\"b\"+str(i)]=params[\"b\"+str(i)]-alpha*grads['db'+str(i)]\n",
        "  return params\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H2vWaEkakmP",
        "colab_type": "text"
      },
      "source": [
        "## 6. Building a n-layered model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHKzBL5maj1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
        "    \"\"\"\n",
        "    Implements a n-layer neural network: [LINEAR->RELU]*(n-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. (≈ 1 line of code)\n",
        "    params = initialize_parameters(layers_dims)\n",
        "    n_layers=len(layers_dims)-1\n",
        "    alpha=learning_rate\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        A, Z = model_forward(X,params)\n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = cal_cost(A[str(n_layers)],Y,1)\n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads,cache = model_backward(Y,A,Z,params)\n",
        " \n",
        "        # Update parameters.\n",
        "        params = update_params(params,grads,alpha,n_layers)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "            costs.append(cost)\n",
        "        \n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return params"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}